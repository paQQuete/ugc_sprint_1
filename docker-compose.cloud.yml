---
version: '2'
services:
  zookeeper-kafka:
    image: confluentinc/cp-zookeeper:6.0.1
    hostname: zookeeper-kafka
    container_name: zookeeper-kafka
    ports:
      - "2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - clickhouse-network
    restart: always
  broker:
    image: confluentinc/cp-server:6.0.1
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper-kafka
    ports:
      - "9092:9092"
      - "9101:9101"
      - "29092:29092"
    networks:
      - clickhouse-network
    restart: always
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-kafka:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://ctube-study.ru:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: ctube-study.ru:29092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

  schema-registry:
    image: confluentinc/cp-schema-registry:6.0.1
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker
    ports:
      - "8081"
    networks:
      - clickhouse-network
    restart: always
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'ctube-study.ru:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  connect:
    image: confluentinc/cp-kafka-connect:6.0.1
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper-kafka
      - broker
    ports:
      - "8083:8083"
    networks:
      - clickhouse-network
    restart: always
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'ctube-study.ru:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR,com.mongodb.kafka=DEBUG"
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # Assumes image is based on confluentinc/kafka-connect-datagen:latest which is pulling 5.2.2 Connect image
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-5.2.2.jar
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
    command: "bash -c 'if [ ! -d /usr/share/confluent-hub-components/confluentinc-kafka-connect-datagen ]; then echo \"WARNING: Did not find directory for kafka-connect-datagen (did you remember to run: docker-compose up -d --build ?)\"; fi ; /etc/confluent/docker/run'"
    volumes:
      - ./mongodb/kafka-connect:/usr/share/confluent-hub-components/kafka-connect-mongodb

  control-center:
    image: confluentinc/cp-enterprise-control-center:6.0.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - broker
      - schema-registry
      - connect
      - ksqldb-server
    ports:
      - "9021:9021"
    networks:
      - clickhouse-network
    restart: always
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'ctube-study.ru:29092'
      CONTROL_CENTER_CONNECT_CLUSTER: 'connect:8083'
      CONTROL_CENTER_KSQL_KSQLDB1_URL: "http://ksqldb-server:8088"
      CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: "http://localhost:8088"
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021

  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:6.0.1
    hostname: ksqldb-server
    container_name: ksqldb-server
    depends_on:
      - broker
      - connect
    ports:
      - "8088"
    networks:
      - clickhouse-network
    restart: always
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_BOOTSTRAP_SERVERS: "ctube-study.ru:29092"
      KSQL_HOST_NAME: ksqldb-server
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_CONNECT_URL: "http://connect:8083"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'

  ksqldb-cli:
    image: confluentinc/cp-ksqldb-cli:6.0.1
    container_name: ksqldb-cli
    depends_on:
      - broker
      - connect
      - ksqldb-server
    entrypoint: /bin/sh
    tty: true
    networks:
      - clickhouse-network
    restart: always

  ksql-datagen:
    image: confluentinc/ksqldb-examples:6.0.1
    hostname: ksql-datagen
    container_name: ksql-datagen
    depends_on:
      - ksqldb-server
      - broker
      - schema-registry
      - connect
    command: "bash -c 'echo Waiting for Kafka to be ready... && \
                       cub kafka-ready -b broker:29092 1 40 && \
                       echo Waiting for Confluent Schema Registry to be ready... && \
                       cub sr-ready schema-registry 8081 40 && \
                       echo Waiting a few seconds for topic creation to finish... && \
                       sleep 11 && \
                       tail -f /dev/null'"
    networks:
      - clickhouse-network
    restart: always
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      STREAMS_BOOTSTRAP_SERVERS: ctube-study.ru:29092
      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry
      STREAMS_SCHEMA_REGISTRY_PORT: 8081

  rest-proxy:
    image: confluentinc/cp-kafka-rest:6.0.1
    depends_on:
      - broker
      - schema-registry
    ports:
      - "8082"
    networks:
      - clickhouse-network
    restart: always
    hostname: rest-proxy
    container_name: rest-proxy
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'ctube-study.ru:29092'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
  zookeeper:
    image: zookeeper:3.5
    container_name: zookeeper
    hostname: zookeeper
    networks:
      - clickhouse-network
    restart: always

  clickhouse-node1:
    image: yandex/clickhouse-server:latest
    container_name: clickhouse-node1
    hostname: clickhouse-node1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./clickhouse/node1:/etc/clickhouse-server
    depends_on:
      - zookeeper
    networks:
      - clickhouse-network
    restart: always

  clickhouse-node2:
    image: yandex/clickhouse-server:latest
    container_name: clickhouse-node2
    hostname: clickhouse-node2
    volumes:
      - ./clickhouse/node2:/etc/clickhouse-server
    depends_on:
      - zookeeper
    networks:
      - clickhouse-network
    restart: always

  clickhouse-node3:
    image: yandex/clickhouse-server:latest
    container_name: clickhouse-node3
    hostname: clickhouse-node3
    volumes:
      - ./clickhouse/node3:/etc/clickhouse-server
    depends_on:
      - zookeeper
    networks:
      - clickhouse-network
    restart: always

  clickhouse-node4:
    image: yandex/clickhouse-server:latest
    container_name: clickhouse-node4
    hostname: clickhouse-node4
    volumes:
      - ./clickhouse/node4:/etc/clickhouse-server
    depends_on:
      - zookeeper
    networks:
      - clickhouse-network
    restart: always

  mongors1n1:
    container_name: mongors1n1
    image: mongo
    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017
    ports:
      - "27017:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/data1:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongors1n2:
    container_name: mongors1n2
    image: mongo
    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017
    ports:
      - "27027:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/data2:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongors1n3:
    container_name: mongors1n3
    image: mongo
    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017
    ports:
      - "27037:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/data3:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongors2n1:
    container_name: mongors2n1
    image: mongo
    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27017
    ports:
      - "27047:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/data4:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongors2n2:
    container_name: mongors2n2
    image: mongo
    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27017
    ports:
      - "27057:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/data5:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongors2n3:
    container_name: mongors2n3
    image: mongo
    command: mongod --shardsvr --replSet mongors2 --dbpath /data/db --port 27017
    ports:
      - "27067:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/data6:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongocfg1:
    container_name: mongocfg1
    image: mongo
    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27017
    expose:
      - "27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/config1:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongocfg2:
    container_name: mongocfg2
    image: mongo
    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27017
    expose:
      - "27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/config2:/data/db
    networks:
      - clickhouse-network
    restart: always

  mongocfg3:
    container_name: mongocfg3
    image: mongo
    command: mongod --configsvr --replSet mongors1conf --dbpath /data/db --port 27017
    expose:
      - "27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /tmp/mongo_cluster/config3:/data/db
    networks:
      - clickhouse-network
    restart: always


  mongos1:
    container_name: mongos1
    image: mongo
    depends_on:
      - mongocfg1
      - mongocfg2
    command: mongos --configdb mongors1conf/mongocfg1:27017,mongocfg2:27017,mongocfg3:27017 --port 27017
    ports:
      - "27019:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
    networks:
      - clickhouse-network
    restart: always

  mongos2:
    container_name: mongos2
    image: mongo
    depends_on:
      - mongocfg1
      - mongocfg2
    command: mongos --configdb mongors1conf/mongocfg1:27017,mongocfg2:27017,mongocfg3:27017 --port 27017
    ports:
      - "27020:27017"
    volumes:
      - /etc/localtime:/etc/localtime:ro
    networks:
      - clickhouse-network
    restart: always

  jupyter:
    image: python:slim
    container_name: jupyter
    command: bash -c "pip install jupyter clickhouse-driver vertica-python pymongo mimesis && jupyter notebook --port=8888 --no-browser --ip=0.0.0.0 --allow-root"
    ports:
      - "8888:8888"
    volumes:
      - ./db_research/volume:/home/user
    restart: always

  nginx:
    image: nginx:1.19.2
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
#    depends_on:
#      - fastapi
    ports:
      - "8080:80"
    logging:
      driver: gelf
      options:
        gelf-address: udp://127.0.0.1:5044
        tag: nginx
    networks:
      - clickhouse-network

  logstash:
    image: logstash:7.10.1
    environment:
      # Так как сейчас вы хотите запустить logstash без Elasticsearch, необходимо отключить встроенный мониторинг, отправляющий данные в ES
      XPACK_MONITORING_ENABLED: "false"
      ES_HOST: "elasticsearch:9200"
    ports:
      - "5044:5044/udp"
    volumes:
      # Монтируем файл с конфигурацией logstash
      - ./logstash/logstash.conf:/config/logstash.conf:ro
    # Запускаем с указанием конфигурационного файла
    command: logstash -f /config/logstash.conf
    networks:
      - clickhouse-network
    restart: always

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2
    environment:
      # Указываем ES запуститься в одном экземпляре
      discovery.type: single-node
    ports:
      - "9200:9200"
    volumes:
      - /tmp/esdata:/tmp/elasticsearch/data
  # Обратите внимание: не стоит использовать для ELK тот же ES, который задействован для полнотекстового поиска в вашем сервисе
    networks:
      - clickhouse-network
    restart: always

  kibana:
    image: docker.elastic.co/kibana/kibana:7.10.2
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - clickhouse-network
    restart: always

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.3.2
    volumes:
      - /tmp/logs/nginx:/var/log/nginx:ro
      - ./beat/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:rw
    depends_on:
      - nginx
      - logstash
      - elasticsearch
      - kibana
      - fastapi
    links:
      - logstash
    networks:
      - clickhouse-network
    restart: always

  fastapi:
    build: ./fastapi
    container_name: ugc_service
    hostname: ugc
    depends_on:
      - zookeeper-kafka
      - zookeeper
    env_file:
      - .env
    ports:
      - "8000:8000"
    networks:
      - clickhouse-network

networks:
  clickhouse-network:
    driver: bridge
    external: false